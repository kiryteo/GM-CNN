model:
  group: "dihedral"
  order: 14
  nbr: 2
  in_channels: 20
  out_channels: 128
  num_blocks: 4
  optimizer:
    name: "AdamW"
    params:
      lr: 0.009
      weight_decay: 0.0153

scheduler:
  mode: "min"
  factor: 0.7
  patience: 3
  verbose: true
  monitor_metric: "valid_rmse"

data:
  batch_size: 32
  num_workers: 4
  input_length: 10
  mid: 22
  output_length: 6
  direc: "/path/to/data"
  train_task: [27, 9, 7, 11, 4, 26, 35, 2, 29, 10, 34, 12, 37, 28, 18, 24, 8, 14, 1, 31, 25, 0, 19, 15, 36, 3, 20, 13]
  valid_task: [27, 9, 7, 11, 4, 26, 35, 2, 29, 10, 34, 12, 37, 28, 18, 24, 8, 14, 1, 31, 25, 0, 19, 15, 36, 3, 20, 13]
  test_future_task: [27, 9, 7, 11, 4, 26, 35, 2, 29, 10, 34, 12, 37, 28, 18, 24, 8, 14, 1, 31, 25, 0, 19, 15, 36, 3, 20, 13]
  test_domain_task: [5, 30, 16, 23, 33, 6, 17, 22, 21, 32]
  train_time: [0, 160]
  valid_time: [160, 200]
  test_future_time: [200, 250]
  test_domain_time: [0, 100]
  stack: true

logger:
  project: "Partial_equivariance"
  log_model: "gradients"

callbacks:
  checkpoint:
    dirpath: "/path/to/checkpoints"
    filename: "best_model"
    monitor: "valid_rmse"
    mode: "min"
  early_stop:
    enabled: true
    monitor: "valid_rmse"
    patience: 6
    mode: "min"

trainer:
  max_epochs: 100
  gpus: 1
  precision: 16
  accelerator: ddp

tester:
  gpus: 1
  precision: 16
  accelerator: ddp
