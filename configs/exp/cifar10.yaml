# Config for ImplicitCNN

#dataset: cifar10

# Model
model:
  group: cyclic
  order: 32
  #err_idx: 511
  nbr: 3
  lr: 0.003
  dataset: cifar10
  num_classes: 10
  dropout: 0.2
  blocks:
    - num_layers: 2
      out_channels: [48, 48]
    - num_layers: 2 
      out_channels: [96, 96]
    - num_layers: 2
      out_channels: [160, 160]

# Data
data:
  path: /localscratch/asa420/data/
  batch_size: 256 
  num_workers: 8

# Training
trainer:
  accelerator: gpu
  devices: 1 
  #strategy: ddp_find_unused_parameters_false
  max_epochs: 100
  #gradient_clip_val: 0.9
  log_every_n_steps: 10

tester:
  accelerator: gpu
  devices: 1
  num_nodes: 1

overfit:
  bs: 1

# Callbacks
#callbacks:
#  model_checkpoint:
#    _target_: pytorch_lightning.callbacks.ModelCheckpoint
#    monitor: val_accuracy
#    mode: max
#
#  lr_monitor:
#    _target_: pytorch_lightning.callbacks.LearningRateMonitor
#    logging_interval: step

# Logger
#logger:
#  wandb:
#    _target_: pytorch_lightning.loggers.WandbLogger
#    project: Partial_equivariance
#    log_model: gradients
